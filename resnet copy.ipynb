{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.datasets import CIFAR10, CIFAR100\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import torch \n",
    "import torch.nn as nn \n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "import matplotlib.pyplot as plt "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    \"device\": \"cuda:5\",\n",
    "    \"data\": {\n",
    "        \"width\": 32, \n",
    "        \"height\": 32\n",
    "    }, \n",
    "    \"train\": {\n",
    "        \"batch_size\": 128, \n",
    "        \"epochs\": 100, \n",
    "        \"num_classes\": 100, \n",
    "        \"dropout\": 0.04, \n",
    "        \"lr\": 0.01, \n",
    "        \"momentum\": 0.9, \n",
    "        \"weight_decay\": 0.00001\n",
    "    }\n",
    "} \n",
    "\n",
    "input_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MeanSubstraction(object): \n",
    "\n",
    "    def __call__(self, imgs): \n",
    "        mean, std = imgs.mean(axis=0), imgs.std(axis=0)\n",
    "        imgs /= 255.0\n",
    "\n",
    "        return imgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "train_transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    MeanSubstraction(),\n",
    "    transforms.Pad(padding=4), \n",
    "    transforms.RandomCrop(config[\"data\"][\"width\"]), \n",
    "    transforms.RandomRotation(10),\n",
    "    transforms.RandomHorizontalFlip(), # 50 % \n",
    "])\n",
    "\n",
    "test_transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "     MeanSubstraction()\n",
    "])\n",
    "\n",
    "\n",
    "train_dataset =  CIFAR100(root=\"../../research/dataset/\", download=True, train=True, transform=train_transform)\n",
    "test_dataset = CIFAR100(root=\"../../research/dataset/\", download=True, train=False, transform=test_transform)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=config[\"train\"][\"batch_size\"], num_workers=4, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=config[\"train\"][\"batch_size\"], num_workers=4)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([128, 3, 32, 32])\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 9 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "### check data \n",
    "\n",
    "for data in train_loader: \n",
    "    imgs, labels = data\n",
    "\n",
    "    print(imgs.shape)\n",
    "    for i in range(9):\n",
    "        ax = plt.subplot(3, 3, i + 1)\n",
    "        np_img = torch.permute(imgs[i], (1, 2, 0)).numpy()\n",
    "        plt.imshow(np_img)\n",
    "        plt.title(int(labels[i]))\n",
    "        plt.axis(\"off\")\n",
    "\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model \n",
    "\n",
    "\n",
    "class ConvModule(nn.Module): \n",
    "\n",
    "    \"\"\"\n",
    "    A simple conv -> bn -> relu block \n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self, \n",
    "        in_c, \n",
    "        out_c, \n",
    "        kernel_size=3,\n",
    "        stride=1, \n",
    "        padding=0, \n",
    "        residual=False\n",
    "    ): \n",
    "        super().__init__()\n",
    "        self.residual = residual\n",
    "        self.conv = nn.Conv2d(in_c, out_c, kernel_size, stride, padding, bias=False)\n",
    "        if not residual: \n",
    "            self.bn = nn.BatchNorm2d(out_c)\n",
    "            self.relu = nn.ReLU()\n",
    "\n",
    "\n",
    "    def forward(self, x): \n",
    "        \n",
    "        out = self.conv(x) \n",
    "\n",
    "        if not self.residual:\n",
    "            out = self.bn(out) \n",
    "            out = self.relu(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "\n",
    "class DepthWiseSeperableConvolution(nn.Module): \n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_c, \n",
    "        out_c, \n",
    "        kernel_size=3, \n",
    "        stride=1, \n",
    "        padding=0 \n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.dwconv = nn.Conv2d(in_c, in_c, kernel_size, stride, padding, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(in_c)\n",
    "        self.relu1 = nn.ReLU()\n",
    "\n",
    "        self.pwconv = nn.Conv2d(in_c, out_c, 1, 1, 0, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(out_c)\n",
    "        self.relu2 = nn.ReLU()\n",
    "\n",
    "    def forward(self, x): \n",
    "\n",
    "        x1 = self.dwconv(x) \n",
    "        x2 = self.bn1(x1)\n",
    "        x3 = self.relu1(x2)\n",
    "\n",
    "        x4 = self.pwconv(x3)\n",
    "        x5 = self.bn2(x4)\n",
    "        x6 = self.relu2(x5)\n",
    "        return x6\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class ResnetBlock2n(nn.Module): \n",
    "    \"\"\"\n",
    "    Simple Resnet layer with 2*2 blocks\n",
    "    block1 (reduce dimension or not) -> block2 -> block3 -> block4\n",
    "    \"\"\"\n",
    "    def __init__(self, in_c, out_c, reduce_dim=False): \n",
    "        super().__init__()\n",
    "\n",
    "        self.reduce_dim = True\n",
    "        stride = 2 if reduce_dim else 1 \n",
    "        self.conv1 = ConvModule(in_c, out_c, 3, stride=stride, padding=1)\n",
    "        self.conv2 = ConvModule(out_c, out_c, 3, 1, 1) \n",
    "        self.conv3 = ConvModule(out_c, out_c, 3, 1, 1) \n",
    "        self.conv4 = ConvModule(out_c, out_c, 3, 1, 1) \n",
    "\n",
    "        self.shortcut = nn.Conv2d(in_c, out_c, 1, stride=stride, padding=0)\n",
    "\n",
    "    def forward(self, x): \n",
    "        \n",
    "        identity = self.shortcut(x)\n",
    "        x1 = self.conv1(x)\n",
    "        x2 = self.conv2(x1)\n",
    "        out = x2 + identity\n",
    "        x3 = self.conv3(out)\n",
    "        x4 = self.conv4(x3)\n",
    "        out = x4 + out \n",
    "        \n",
    "        return out\n",
    "        \n",
    "\n",
    "\n",
    "class ResnetBlock3n(ResnetBlock2n):\n",
    "    \"\"\"\n",
    "    Simple Resnet layer with 3*2 blocks. \n",
    "    This class extends the above ResnetBlock2n class\n",
    "    \"\"\"\n",
    "    def __init__(self, in_c, out_c, reduce_dim=False): \n",
    "        super().__init__(in_c, out_c, reduce_dim)\n",
    "\n",
    "        self.conv5 = ConvModule(out_c, out_c, 3, 1, 1, residual=False)\n",
    "        self.conv6 = ConvModule(out_c, out_c, 3, 1, 1, residual=False) \n",
    "\n",
    "    def forward(self, x): \n",
    "        x4 = super().forward(x) \n",
    "        x5 = self.conv5(x4)\n",
    "        x6 = self.conv6(x5)\n",
    "\n",
    "        return x6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNet_18_cifar(nn.Module): \n",
    "\n",
    "    def __init__(self, dropout, nb_classes): \n",
    "\n",
    "        super().__init__()\n",
    "        self.block1 = ResnetBlock3n(3, 32)\n",
    "        self.block2 = ResnetBlock3n(32, 64, reduce_dim=True)\n",
    "        self.block3 = ResnetBlock3n(64, 128, reduce_dim=True)\n",
    "\n",
    "        self.pool = nn.AvgPool2d(2, 2)\n",
    "        self.fc = nn.Linear(128*4*4, nb_classes)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "\n",
    "    def initialize_weights(self):\n",
    "        # Loop over the modules in the model\n",
    "        for module in self.modules():\n",
    "            # If the module is a linear or convolutional layer\n",
    "            if isinstance(module, (torch.nn.Linear, torch.nn.Conv2d)):\n",
    "                torch.nn.init.kaiming_uniform_(module.weight, leaky_relu=\"relu\")\n",
    "                torch.nn.init.constant_(module.bias, 0)\n",
    "\n",
    "    def forward(self, x): \n",
    "        bs = x.shape[0]\n",
    "        x1 = self.block1(x) \n",
    "        x2 = self.block2(x1)\n",
    "        x3 = self.block3(x2)\n",
    "        out = self.pool(x3) \n",
    "        out = out.view(bs, -1)\n",
    "        out = self.dropout(out)\n",
    "        out = self.fc(out) \n",
    "\n",
    "        return out\n",
    "\n",
    "        \n",
    "\n",
    "class ResNet_24_cifar(nn.Module): \n",
    "\n",
    "    def __init__(self, dropout, nb_classes, *args, **kwargs): \n",
    "\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.block1 = ResnetBlock3n(3, 16)\n",
    "        self.block2 = ResnetBlock3n(16, 32, reduce_dim=True)\n",
    "        self.block3 = ResnetBlock3n(32, 32, reduce_dim=False)\n",
    "        self.block4 = ResnetBlock3n(32, 64, reduce_dim=True)\n",
    "\n",
    "        self.pool = nn.AvgPool2d(2, 2)\n",
    "        self.fc = nn.Linear(64*4*4, nb_classes)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x): \n",
    "        bs = x.shape[0]\n",
    "        x1 = self.block1(x) \n",
    "        x2 = self.block2(x1) \n",
    "        x3 = self.block3(x2)\n",
    "        x4 = self.block4(x3)\n",
    "        out = self.pool(x4) \n",
    "        out = out.view(bs, -1)\n",
    "        out = self.dropout(out)\n",
    "        out = self.fc(out) \n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "class ResNet_42_cifar(nn.Module): \n",
    "\n",
    "    def __init__(self, dropout, nb_classes, *args, **kwargs): \n",
    "\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.block1 = ResnetBlock3n(3, 32)\n",
    "        self.block2 = ResnetBlock3n(32, 32, reduce_dim=False)\n",
    "        self.block3 = ResnetBlock3n(32, 32, reduce_dim=True)\n",
    "        self.block4 = ResnetBlock3n(32, 64, reduce_dim=False)\n",
    "        self.block5 = ResnetBlock3n(64, 128, reduce_dim=True)\n",
    "        self.block6 = ResnetBlock3n(128, 128, reduce_dim=False)\n",
    "\n",
    "        self.pool = nn.AvgPool2d(2, 2)\n",
    "        self.fc = nn.Linear(128*4*4, nb_classes)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x): \n",
    "        bs = x.shape[0]\n",
    "        x1 = self.block1(x) \n",
    "        x2 = self.block2(x1) \n",
    "        x3 = self.block3(x2)\n",
    "        x4 = self.block4(x3)\n",
    "        x5 = self.block5(x4) \n",
    "        x6 = self.block6(x5)\n",
    "        out = self.pool(x6) \n",
    "        out = out.view(bs, -1)\n",
    "        out = self.dropout(out)\n",
    "        out = self.fc(out) \n",
    "\n",
    "        return out\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(logits, target):\n",
    "\n",
    "    preds = nn.functional.softmax(logits) \n",
    "    preds = torch.argmax(preds, axis=1) \n",
    "    acc = torch.where(preds == target, 1, 0).to(torch.float32).mean()\n",
    "    return acc\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Loop "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, optimizer, criterion, train_loader, epoch, writer=None, scheduler=None): \n",
    "\n",
    "    if not writer: \n",
    "        print(\"No writer selected, tensorboard will not be used\")\n",
    "    \n",
    "    if not scheduler: \n",
    "        print(\"No learning rate scheduler\") \n",
    "\n",
    "    \n",
    "    total_loss = 0.0\n",
    "    total_acc = 0.0\n",
    "    for i, data in enumerate(train_loader): \n",
    "        \n",
    "        model.zero_grad()\n",
    "        imgs, labels = data \n",
    "        # fix use of global variable \n",
    "        imgs = imgs.to(config[\"device\"])\n",
    "        labels = labels.to(config[\"device\"])\n",
    "\n",
    "        predictions = model(imgs) \n",
    "        loss = criterion(predictions, labels)   \n",
    "        total_loss += loss.item()\n",
    "        total_acc += accuracy(predictions, labels)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step() \n",
    "\n",
    "\n",
    "        if i % 100 == 0 and i != 0: \n",
    "            # print(f\"[EPOCH {epoch} | {i} / {len(train_loader)}] : cross entropy = {total_loss / 100}\")\n",
    "            grad_sum = 0.0 \n",
    "\n",
    "            for name, p in model.named_parameters(): \n",
    "                norm = p.grad.data.norm(2)\n",
    "                grad_sum += norm.item() ** 2\n",
    "\n",
    "            grad_sum = grad_sum ** (1. / 2)\n",
    "            if writer:\n",
    "                writer.add_scalar(\"Train/loss\", total_loss / 100, epoch*len(train_loader) + i + 1) \n",
    "                writer.add_scalar(\"Train/gradient_norm\", grad_sum, epoch*len(train_loader) + i + 1)\n",
    "                writer.add_scalar(\"Train/accuracy\", total_acc / 100, epoch*len(train_loader) + i + 1)\n",
    "\n",
    "            total_loss = 0.0\n",
    "            total_acc = 0.0\n",
    "\n",
    "\n",
    "    if scheduler is not None: \n",
    "        scheduler.step()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def test(model, criterion, scheduler, test_loader, epoch, writer=None): \n",
    "\n",
    "    validation_loss = 0.0 \n",
    "    total_acc = 0.0\n",
    "    lenght = len(test_loader)\n",
    "\n",
    "    for i, data in enumerate(test_loader):\n",
    "\n",
    "        imgs, labels = data \n",
    "        imgs = imgs.to(config[\"device\"])\n",
    "        labels = labels.to(config[\"device\"])\n",
    "\n",
    "        predictions = model(imgs) \n",
    "        loss = criterion(predictions, labels)   \n",
    "        validation_loss += loss.item()\n",
    "        total_acc += accuracy(predictions, labels)\n",
    "    \n",
    "    if writer is not None: \n",
    "        writer.add_scalar(\"Test/loss\", validation_loss / lenght, epoch*lenght + i + 1)\n",
    "        writer.add_scalar(\"Test/accuracy\", total_acc / lenght, epoch*lenght + i + 1)\n",
    "        if scheduler is not None: \n",
    "            writer.add_scalar(\"Train/Learning rate\", scheduler.get_last_lr()[0], epoch)\n",
    "        else: \n",
    "            writer.add_scalar(\"Train/Learning rate\", config[\"train\"][\"lr\"], epoch)\n",
    "\n",
    "    print(f\"Validation at EPOCH={epoch} : \", validation_loss / lenght)\n",
    "    print(\"Last learning rate : \", scheduler.get_last_lr()[0])\n",
    "\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_loop(model, optimizer, criterion, train_loader, writer=None, scheduler=None):\n",
    "    \n",
    "    print(\"--- Training ---\")\n",
    "    model.train()\n",
    "    for epoch in range(config[\"train\"][\"epochs\"]): \n",
    "        train(model, optimizer, criterion, train_loader, epoch, writer, scheduler)\n",
    "        test(model, criterion, scheduler, test_loader, epoch, writer)\n",
    "    print(\"--- Training : DONE ---\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(): \n",
    "    model = ResNet_42_cifar(dropout=config[\"train\"][\"dropout\"], nb_classes=config[\"train\"][\"num_classes\"])\n",
    "    model = model.to(config[\"device\"])\n",
    "    print(\"device : \", next(model.parameters()).device)\n",
    "    # print(summary(model, (3, config[\"data\"][\"width\"], config[\"data\"][\"height\"])))\n",
    "    return model \n",
    "\n",
    "\n",
    "def get_optimizer(model): \n",
    "    optimizer = torch.optim.SGD(\n",
    "        model.parameters(), \n",
    "        lr=config[\"train\"][\"lr\"],\n",
    "        momentum=config[\"train\"][\"momentum\"],\n",
    "        weight_decay=config[\"train\"][\"weight_decay\"]\n",
    "    )\n",
    "    return optimizer \n",
    "\n",
    "def get_scheduler(optimizer): \n",
    "    scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=[15, 20, 25, 30], gamma=0.1)\n",
    "    return scheduler\n",
    "\n",
    "\n",
    "def get_criterion():\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    return criterion "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(): \n",
    "\n",
    "    print(\"--- Loading model ---\")\n",
    "    model = get_model()\n",
    "    print(\"--- Loading model : DONE ---\") \n",
    "\n",
    "    print(\"--- Loading optimizer ---\")\n",
    "    optimizer = get_optimizer(model)\n",
    "    print(\"--- Loading optimizer : DONE ---\")\n",
    "\n",
    "    print(\"--- Loading scheduler ---\")\n",
    "    scheduler = get_scheduler(optimizer)\n",
    "    print(\"--- Loading scheduler : DONE ---\")\n",
    "\n",
    "    print(\"--- Loading criterion ---\")\n",
    "    criterion = get_criterion() \n",
    "    print(\"--- Load criterion : DONE ---\")\n",
    "\n",
    "    config_name = f\"{type(model).__name__}_\\\n",
    "        lr={config['train']['lr']}_\\\n",
    "        bs={config['train']['batch_size']}\\\n",
    "        optimizer={type(optimizer).__name__}\\\n",
    "        scheduler={type(scheduler).__name__}\\\n",
    "        filter_size={(32, 32, 64, 128, 128)}\\\n",
    "        momentum={config['train']['momentum']}\\\n",
    "        cifar100\\\n",
    "        weight_decay={config['train']['weight_decay']}\\\n",
    "    \"\n",
    "    writer = SummaryWriter(\"runs/\" + config_name)\n",
    "\n",
    "    # train \n",
    "    training_loop(model, optimizer, criterion, train_loader, writer, scheduler)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Loading model ---\n",
      "device :  cuda:5\n",
      "--- Loading model : DONE ---\n",
      "--- Loading optimizer ---\n",
      "--- Loading optimizer : DONE ---\n",
      "--- Loading scheduler ---\n",
      "--- Loading scheduler : DONE ---\n",
      "--- Loading criterion ---\n",
      "--- Load criterion : DONE ---\n",
      "--- Training ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3331287/1052425669.py:3: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  preds = nn.functional.softmax(logits)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation at EPOCH=0 :  3.8387832490703726\n",
      "Last learning rate :  0.01\n",
      "Validation at EPOCH=1 :  3.365895506701892\n",
      "Last learning rate :  0.01\n",
      "Validation at EPOCH=2 :  3.136415049999575\n",
      "Last learning rate :  0.01\n",
      "Validation at EPOCH=3 :  2.86370197127137\n",
      "Last learning rate :  0.01\n",
      "Validation at EPOCH=4 :  2.664069112343124\n",
      "Last learning rate :  0.01\n",
      "Validation at EPOCH=5 :  2.555677311329902\n",
      "Last learning rate :  0.01\n",
      "Validation at EPOCH=6 :  2.441776957692979\n",
      "Last learning rate :  0.01\n",
      "Validation at EPOCH=7 :  2.291178138950203\n",
      "Last learning rate :  0.01\n",
      "Validation at EPOCH=8 :  2.1865739520592027\n",
      "Last learning rate :  0.01\n",
      "Validation at EPOCH=9 :  2.142660427697097\n",
      "Last learning rate :  0.01\n",
      "Validation at EPOCH=10 :  2.065609440018859\n",
      "Last learning rate :  0.01\n",
      "Validation at EPOCH=11 :  2.026406722732737\n",
      "Last learning rate :  0.01\n",
      "Validation at EPOCH=12 :  1.9681579221653034\n",
      "Last learning rate :  0.01\n",
      "Validation at EPOCH=13 :  1.918784473515764\n",
      "Last learning rate :  0.01\n",
      "Validation at EPOCH=14 :  1.867973830126509\n",
      "Last learning rate :  0.001\n",
      "Validation at EPOCH=15 :  1.7261416082140766\n",
      "Last learning rate :  0.001\n",
      "Validation at EPOCH=16 :  1.7003296689142156\n",
      "Last learning rate :  0.001\n",
      "Validation at EPOCH=17 :  1.6931109911278834\n",
      "Last learning rate :  0.001\n",
      "Validation at EPOCH=18 :  1.67948318433158\n",
      "Last learning rate :  0.001\n",
      "Validation at EPOCH=19 :  1.670678669893289\n",
      "Last learning rate :  0.0001\n",
      "Validation at EPOCH=20 :  1.6655748463884186\n",
      "Last learning rate :  0.0001\n",
      "Validation at EPOCH=21 :  1.6600211587133287\n",
      "Last learning rate :  0.0001\n",
      "Validation at EPOCH=22 :  1.663167776940744\n",
      "Last learning rate :  0.0001\n",
      "Validation at EPOCH=23 :  1.6566405960276156\n",
      "Last learning rate :  0.0001\n",
      "Validation at EPOCH=24 :  1.656567715391328\n",
      "Last learning rate :  1e-05\n",
      "Validation at EPOCH=25 :  1.658675863773008\n",
      "Last learning rate :  1e-05\n",
      "Validation at EPOCH=26 :  1.6618795243999627\n",
      "Last learning rate :  1e-05\n",
      "Validation at EPOCH=27 :  1.6606632984137233\n",
      "Last learning rate :  1e-05\n",
      "Validation at EPOCH=28 :  1.661234048348439\n",
      "Last learning rate :  1e-05\n",
      "Validation at EPOCH=29 :  1.6601323538188693\n",
      "Last learning rate :  1.0000000000000002e-06\n",
      "Validation at EPOCH=30 :  1.6611017803602581\n",
      "Last learning rate :  1.0000000000000002e-06\n",
      "Validation at EPOCH=31 :  1.6601655241809314\n",
      "Last learning rate :  1.0000000000000002e-06\n",
      "Validation at EPOCH=32 :  1.6577104239524165\n",
      "Last learning rate :  1.0000000000000002e-06\n",
      "Validation at EPOCH=33 :  1.658598148370091\n",
      "Last learning rate :  1.0000000000000002e-06\n",
      "Validation at EPOCH=34 :  1.6601894082902353\n",
      "Last learning rate :  1.0000000000000002e-06\n",
      "Validation at EPOCH=35 :  1.657446346705473\n",
      "Last learning rate :  1.0000000000000002e-06\n",
      "Validation at EPOCH=36 :  1.6591511180129233\n",
      "Last learning rate :  1.0000000000000002e-06\n",
      "Validation at EPOCH=37 :  1.659741335277316\n",
      "Last learning rate :  1.0000000000000002e-06\n",
      "Validation at EPOCH=38 :  1.6571708298936676\n",
      "Last learning rate :  1.0000000000000002e-06\n",
      "Validation at EPOCH=39 :  1.6564109612114821\n",
      "Last learning rate :  1.0000000000000002e-06\n",
      "Validation at EPOCH=40 :  1.6581488244141205\n",
      "Last learning rate :  1.0000000000000002e-06\n",
      "Validation at EPOCH=41 :  1.659882353830941\n",
      "Last learning rate :  1.0000000000000002e-06\n",
      "Validation at EPOCH=42 :  1.6583922935437552\n",
      "Last learning rate :  1.0000000000000002e-06\n",
      "Validation at EPOCH=43 :  1.659200607975827\n",
      "Last learning rate :  1.0000000000000002e-06\n",
      "Validation at EPOCH=44 :  1.657114314127572\n",
      "Last learning rate :  1.0000000000000002e-06\n",
      "Validation at EPOCH=45 :  1.6585527764091008\n",
      "Last learning rate :  1.0000000000000002e-06\n",
      "Validation at EPOCH=46 :  1.6585482343842712\n",
      "Last learning rate :  1.0000000000000002e-06\n",
      "Validation at EPOCH=47 :  1.6594924292986906\n",
      "Last learning rate :  1.0000000000000002e-06\n",
      "Validation at EPOCH=48 :  1.6591791955730584\n",
      "Last learning rate :  1.0000000000000002e-06\n",
      "Validation at EPOCH=49 :  1.6541699125796934\n",
      "Last learning rate :  1.0000000000000002e-06\n",
      "Validation at EPOCH=50 :  1.658962746209736\n",
      "Last learning rate :  1.0000000000000002e-06\n",
      "Validation at EPOCH=51 :  1.658807934085025\n",
      "Last learning rate :  1.0000000000000002e-06\n",
      "Validation at EPOCH=52 :  1.6614627792865415\n",
      "Last learning rate :  1.0000000000000002e-06\n",
      "Validation at EPOCH=53 :  1.6608692981019806\n",
      "Last learning rate :  1.0000000000000002e-06\n",
      "Validation at EPOCH=54 :  1.6607853611813317\n",
      "Last learning rate :  1.0000000000000002e-06\n",
      "Validation at EPOCH=55 :  1.658555498606042\n",
      "Last learning rate :  1.0000000000000002e-06\n",
      "Validation at EPOCH=56 :  1.6579584079452707\n",
      "Last learning rate :  1.0000000000000002e-06\n",
      "Validation at EPOCH=57 :  1.6522354385520839\n",
      "Last learning rate :  1.0000000000000002e-06\n",
      "Validation at EPOCH=58 :  1.6603220927564404\n",
      "Last learning rate :  1.0000000000000002e-06\n",
      "Validation at EPOCH=59 :  1.658500754380528\n",
      "Last learning rate :  1.0000000000000002e-06\n",
      "Validation at EPOCH=60 :  1.65689657609674\n",
      "Last learning rate :  1.0000000000000002e-06\n",
      "Validation at EPOCH=61 :  1.6580118982097771\n",
      "Last learning rate :  1.0000000000000002e-06\n",
      "Validation at EPOCH=62 :  1.6578227145762383\n",
      "Last learning rate :  1.0000000000000002e-06\n",
      "Validation at EPOCH=63 :  1.6601344618616225\n",
      "Last learning rate :  1.0000000000000002e-06\n",
      "Validation at EPOCH=64 :  1.6607987488372415\n",
      "Last learning rate :  1.0000000000000002e-06\n",
      "Validation at EPOCH=65 :  1.657208835022359\n",
      "Last learning rate :  1.0000000000000002e-06\n",
      "Validation at EPOCH=66 :  1.6584323840805246\n",
      "Last learning rate :  1.0000000000000002e-06\n",
      "Validation at EPOCH=67 :  1.6598866785628885\n",
      "Last learning rate :  1.0000000000000002e-06\n",
      "Validation at EPOCH=68 :  1.6596503212482114\n",
      "Last learning rate :  1.0000000000000002e-06\n",
      "Validation at EPOCH=69 :  1.657340043707739\n",
      "Last learning rate :  1.0000000000000002e-06\n",
      "Validation at EPOCH=70 :  1.6609952676145336\n",
      "Last learning rate :  1.0000000000000002e-06\n",
      "Validation at EPOCH=71 :  1.6589891322051422\n",
      "Last learning rate :  1.0000000000000002e-06\n",
      "Validation at EPOCH=72 :  1.6590485859520827\n",
      "Last learning rate :  1.0000000000000002e-06\n",
      "Validation at EPOCH=73 :  1.6584462500825714\n",
      "Last learning rate :  1.0000000000000002e-06\n",
      "Validation at EPOCH=74 :  1.6571099924135813\n",
      "Last learning rate :  1.0000000000000002e-06\n",
      "Validation at EPOCH=75 :  1.6589025470274914\n",
      "Last learning rate :  1.0000000000000002e-06\n",
      "Validation at EPOCH=76 :  1.6587280306635024\n",
      "Last learning rate :  1.0000000000000002e-06\n",
      "Validation at EPOCH=77 :  1.6591750277748591\n",
      "Last learning rate :  1.0000000000000002e-06\n",
      "Validation at EPOCH=78 :  1.657769894298119\n",
      "Last learning rate :  1.0000000000000002e-06\n",
      "Validation at EPOCH=79 :  1.6586708554738685\n",
      "Last learning rate :  1.0000000000000002e-06\n",
      "Validation at EPOCH=80 :  1.6581053613107415\n",
      "Last learning rate :  1.0000000000000002e-06\n",
      "Validation at EPOCH=81 :  1.6597582252719734\n",
      "Last learning rate :  1.0000000000000002e-06\n",
      "Validation at EPOCH=82 :  1.6601899228518522\n",
      "Last learning rate :  1.0000000000000002e-06\n",
      "Validation at EPOCH=83 :  1.6592202412931225\n",
      "Last learning rate :  1.0000000000000002e-06\n",
      "Validation at EPOCH=84 :  1.6603861322885827\n",
      "Last learning rate :  1.0000000000000002e-06\n",
      "Validation at EPOCH=85 :  1.659402202956284\n",
      "Last learning rate :  1.0000000000000002e-06\n",
      "Validation at EPOCH=86 :  1.6594622104982786\n",
      "Last learning rate :  1.0000000000000002e-06\n",
      "Validation at EPOCH=87 :  1.65998438491097\n",
      "Last learning rate :  1.0000000000000002e-06\n",
      "Validation at EPOCH=88 :  1.6565835792807084\n",
      "Last learning rate :  1.0000000000000002e-06\n",
      "Validation at EPOCH=89 :  1.6617188529123235\n",
      "Last learning rate :  1.0000000000000002e-06\n",
      "Validation at EPOCH=90 :  1.66179032718079\n",
      "Last learning rate :  1.0000000000000002e-06\n",
      "Validation at EPOCH=91 :  1.65830821628812\n",
      "Last learning rate :  1.0000000000000002e-06\n",
      "Validation at EPOCH=92 :  1.6609316566322423\n",
      "Last learning rate :  1.0000000000000002e-06\n",
      "Validation at EPOCH=93 :  1.659601875498325\n",
      "Last learning rate :  1.0000000000000002e-06\n",
      "Validation at EPOCH=94 :  1.6581782872163797\n",
      "Last learning rate :  1.0000000000000002e-06\n",
      "Validation at EPOCH=95 :  1.661304780199558\n",
      "Last learning rate :  1.0000000000000002e-06\n",
      "Validation at EPOCH=96 :  1.659371148181867\n",
      "Last learning rate :  1.0000000000000002e-06\n",
      "Validation at EPOCH=97 :  1.6580574829367143\n",
      "Last learning rate :  1.0000000000000002e-06\n",
      "Validation at EPOCH=98 :  1.6574407423598856\n",
      "Last learning rate :  1.0000000000000002e-06\n",
      "Validation at EPOCH=99 :  1.6585993917682502\n",
      "Last learning rate :  1.0000000000000002e-06\n",
      "--- Training : DONE ---\n"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.5 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d396323f6dd81c22cfb4128ca65135261b7151fc33799cbbdab8dfd4c9620729"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
